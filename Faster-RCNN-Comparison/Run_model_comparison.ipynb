{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "\n",
    "import transforms as T\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TORCH_DEVICE = 'mps' # there is currently a bug: https://github.com/pytorch/pytorch/issues/78915\n",
    "TORCH_DEVICE = 'cpu'\n",
    "# TORCH_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_PATH = './Data/'\n",
    "IMAGE_PATH = '../RPN_Backbone_GZ2/Data/real_pngs/'\n",
    "PRE_TRAINED_MODELS_PATH = '../Faster_R-CNN_GZ2/pre_trained_models/'\n",
    "\n",
    "NUM_CLASSES = 3 # 2 classes (clump, odd clump) + background\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "CUTOUT = (100, 100, 300, 300)\n",
    "CUTOUT_ARRAY = np.array([100, 300, 100, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1672212957006,
     "user": {
      "displayName": "Jurgen Popp",
      "userId": "11749526926963684056"
     },
     "user_tz": 0
    },
    "id": "6MrzmG5hm1ch"
   },
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes=3, trainable_layers=0):\n",
    "    \"\"\"\n",
    "    Creates the model object for Faster R-CNN\n",
    "\n",
    "    Args:\n",
    "      model_name (str): 'Zoobot_pre_trained', 'Zoobot_fine_tuned', 'Resnet_Imagenet'\n",
    "      num_classes (int): number of classes the detector should outpub, \n",
    "        must include a class for the background\n",
    "      trainable_layers (int): number of blocks of the classification backbone,\n",
    "        counted from top, that should be made trainable\n",
    "        e.g. 0 - all blocks fixed, 1 - 'backbone.body.conv1' trainable\n",
    "\n",
    "    Returns:\n",
    "      FasterRCNN model\n",
    "\n",
    "    \"\"\"\n",
    "    import copy_zoobot_weights\n",
    "\n",
    "    # load an object detection model pre-trained on COCO, all layers fixed\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights_backbone='IMAGENET1K_V1',\n",
    "        trainable_backbone_layers=0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if model_name == 'Zoobot_pre_trained':\n",
    "            zoobot_ckpt_path = PRE_TRAINED_MODELS_PATH + 'Zoobot_Resnet_Torchvision/epoch=20-step=6552.ckpt'\n",
    "            model = copy_zoobot_weights.copy_Zoobot_weights_to_Resnet(\n",
    "                model=model, \n",
    "                ckpt_path=zoobot_ckpt_path,\n",
    "                device=TORCH_DEVICE,\n",
    "                trainable_layers=trainable_layers\n",
    "            )\n",
    "            print('Zoobot pre-trained loaded.')\n",
    "    \n",
    "        elif model_name == 'Zoobot_fine_tuned':\n",
    "            zoobot_ckpt_path = PRE_TRAINED_MODELS_PATH + 'Zoobot_Clumps_Resnet/Zoobot_Clump_Classifier_36.pth'\n",
    "            model = copy_zoobot_weights.copy_Zoobot_clumps_weights_to_Resnet(\n",
    "                model=model, \n",
    "                ckpt_path=zoobot_ckpt_path,\n",
    "                device=TORCH_DEVICE,\n",
    "                trainable_layers=trainable_layers\n",
    "            )\n",
    "            print('Zoobot fine-tuned for clumps loaded.')\n",
    "        \n",
    "        elif model_name == 'Resnet_Imagenet':\n",
    "            print('ResNet initialised with Imagenet weights loaded.')\n",
    "    \n",
    "        else:\n",
    "            print('None of the valid models chosen.')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model dict\n",
    "model_dict = {\n",
    "    'resnet' : {\n",
    "        'model_name' : 'Resnet_Imagenet',\n",
    "        'description' : 'ResNet50 initialised with default weights IMAGENET1K_V1',\n",
    "        'log_dir' : './models/FRCNN_Resnet_Imagenet/',\n",
    "    },\n",
    "    'zoobot_clumps' : {\n",
    "        'model_name' : 'Zoobot_fine_tuned',\n",
    "        'description' : 'ResNet50 initialised with weights from a Zoobot classifier fine-tuned for clumps',\n",
    "        'log_dir' : './models/FRCNN_Resnet_Zoobot_Clumps/',\n",
    "    },\n",
    "    'zoobot' : {\n",
    "        'model_name' : 'Zoobot_pre_trained',\n",
    "        'description' : 'ResNet50 initialised with weights from Zoobot, all layers kept fix for training',\n",
    "        'log_dir' : './models/FRCNN_Resnet_Zoobot/',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    augs = []\n",
    "\n",
    "    augs.append(T.PILToTensor())\n",
    "    augs.append(T.ConvertImageDtype(torch.float))\n",
    "    \n",
    "    if train:\n",
    "        augs.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    return T.Compose(augs)\n",
    "\n",
    "\n",
    "def get_dataloader_dict(train_df, val_df, image_dir, cutout, is_colour):\n",
    "    import SDSSGalaxyDataset\n",
    "    image_datasets = {}\n",
    "\n",
    "    image_datasets['train'] = SDSSGalaxyDataset.SDSSGalaxyDataset(\n",
    "        dataframe=train_df,\n",
    "        image_dir=image_dir,\n",
    "        cutout=cutout,\n",
    "        colour=is_colour,\n",
    "        transforms=get_transform(train=True)\n",
    "    )\n",
    "    image_datasets['val'] = SDSSGalaxyDataset.SDSSGalaxyDataset(\n",
    "        dataframe=val_df,\n",
    "        image_dir=image_dir,\n",
    "        cutout=cutout,\n",
    "        colour=is_colour,\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "    \n",
    "    return {x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        collate_fn=utils.collate_fn\n",
    "    ) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the image-to-run relation\n",
    "df_runs = pd.read_pickle(DATA_PATH + 'image_ids_for_runs.pkl')\n",
    "\n",
    "# read the full set with bounding boxes\n",
    "df = pd.read_pickle(DATA_PATH + 'clump_scout_full_set.pkl').rename(columns={'local_id': 'local_ids'})\n",
    "df = df[['zoo_id', 'local_ids', 'label', 'label_text', 'x1', 'x2', 'y1', 'y2']]\n",
    "df['local_ids'] = df['local_ids'].astype(int)\n",
    "df['label'] = df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = df_runs['run'].unique()\n",
    "groups = ['Training', 'Validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = (\n",
    "    df_runs[df_runs['group'].isin(groups)]\n",
    "    .merge(df, how='inner', on='zoo_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataloader\n",
    "run = 1\n",
    "\n",
    "dataloader_dict = get_dataloader_dict(\n",
    "    train_df=df_data[(df_data['run']==run) & (df_data['group']=='Training')],\n",
    "    val_df=df_data[(df_data['run']==run) & (df_data['group']=='Validation')],\n",
    "    image_dir=IMAGE_PATH,\n",
    "    cutout=CUTOUT,\n",
    "    is_colour=True\n",
    ")\n",
    "\n",
    "images, targets = next(iter(dataloader_dict['train']))\n",
    "images = list(image.to(TORCH_DEVICE) for image in images)\n",
    "targets = [{k: v.to(TORCH_DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "for i in range(4):\n",
    "    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,  # the image is in RGB, convert to BGR for cv2 annotations\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (0, 0, 255), 1)\n",
    "    plt.imshow(sample[:, :, ::-1])\n",
    "    # plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "for run in runs:  #for run in range(17,40,1):\n",
    "    print('Executing run: {}'.format(run))\n",
    "    \n",
    "    # load data\n",
    "    dataloader_dict = get_dataloader_dict(\n",
    "        train_df=df_data[(df_data['run']==run) & (df_data['group']=='Training')],\n",
    "        val_df=df_data[(df_data['run']==run) & (df_data['group']=='Validation')],\n",
    "        image_dir=IMAGE_PATH,\n",
    "        cutout=CUTOUT,\n",
    "        is_colour=True\n",
    "    )\n",
    "\n",
    "    for model, model_data in model_dict.items():\n",
    "        # initialise Tensorboard writer\n",
    "        tb_log_dir = model_data['log_dir'] + 'run={}/'.format(run) + 'train'\n",
    "        writer = SummaryWriter(log_dir=tb_log_dir)\n",
    "\n",
    "        # get the model\n",
    "        frcnn_model = get_model(\n",
    "            model_name=model_data['model_name'],\n",
    "            num_classes=NUM_CLASSES,\n",
    "            trainable_layers=0\n",
    "        )\n",
    "        \n",
    "        # move model to the right device\n",
    "        frcnn_model = frcnn_model.to(TORCH_DEVICE)\n",
    "        \n",
    "        # construct an optimizer\n",
    "        params = [p for p in frcnn_model.parameters() if p.requires_grad]\n",
    "        # optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "        optimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=0.00005)\n",
    "        \n",
    "        # and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "        # Looping through epochs\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # train for one epoch, printing every 10 iterations\n",
    "            train_one_epoch(\n",
    "                frcnn_model, \n",
    "                optimizer, \n",
    "                dataloader_dict['train'], \n",
    "                TORCH_DEVICE, \n",
    "                epoch, \n",
    "                print_freq=10,\n",
    "                scaler=None,\n",
    "                tb_writer=writer\n",
    "                # tb_writer=None\n",
    "            )\n",
    "            \n",
    "            # update the learning rate\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # evaluate on the test dataset\n",
    "            coco_evaluator = evaluate(\n",
    "                frcnn_model, \n",
    "                dataloader_dict['val'], \n",
    "                device=TORCH_DEVICE\n",
    "            )\n",
    "            for iou_type, coco_eval in coco_evaluator.coco_eval.items():\n",
    "                writer.add_scalar(\"AP/IoU/0.50-0.95/all/100\", coco_eval.stats[0], epoch)\n",
    "                writer.add_scalar(\"AP/IoU/0.50/all/100\", coco_eval.stats[1], epoch)\n",
    "                writer.add_scalar(\"AP/IoU/0.75/all/100\", coco_eval.stats[2], epoch)\n",
    "                writer.add_scalar(\"AP/IoU/0.50-0.95/small/100\", coco_eval.stats[3], epoch)\n",
    "                writer.add_scalar(\"AP/IoU/0.50-0.95/medium/100\", coco_eval.stats[4], epoch)\n",
    "                writer.add_scalar(\"AP/IoU/0.50-0.95/large/100\", coco_eval.stats[5], epoch)\n",
    "                writer.add_scalar(\"AR/IoU/0.50-0.95/all/1\", coco_eval.stats[6], epoch)\n",
    "                writer.add_scalar(\"AR/IoU/0.50-0.95/all/10\", coco_eval.stats[7], epoch)\n",
    "                writer.add_scalar(\"AR/IoU/0.50-0.95/all/100\", coco_eval.stats[8], epoch)\n",
    "                writer.add_scalar(\"AR/IoU/0.50-0.95/small/100\", coco_eval.stats[9], epoch)\n",
    "                writer.add_scalar(\"AR/IoU/0.50-0.95/medium/100\", coco_eval.stats[10], epoch)\n",
    "                writer.add_scalar(\"AR/IoU/0.50-0.95/large/100\", coco_eval.stats[11], epoch)\n",
    "        \n",
    "            if (epoch+1) % 20 == 0:\n",
    "                model_save_path = model_data['log_dir'] + 'run={}/'.format(run) + model_data['model_name'] + '_{}.pth'.format(epoch+1)\n",
    "                torch.save(frcnn_model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c186b54c8cf1ccf367fb8bdf3e071efe85839a5daed090332edb040d14e1fa50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
