{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transforms as T\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TORCH_DEVICE = 'mps' # there is currently a bug: https://github.com/pytorch/pytorch/issues/78915\n",
    "TORCH_DEVICE = 'cpu'\n",
    "CKPT_PATH = './pre_trained_models/Zoobot_Resnet_Torchvision/'\n",
    "CKPT_NAME = 'epoch=20-step=6552.ckpt'\n",
    "\n",
    "DATA_PATH = '../RPN_Backbone_GZ2/Data/'\n",
    "IMAGE_PATH = DATA_PATH + 'real_pngs/'\n",
    "\n",
    "LOG_DIR = './models/Zoobot_Resnet/train'\n",
    "\n",
    "# using typical split of 80:10:10\n",
    "SIZE_OF_VALIDATION_SET = 0.1\n",
    "SIZE_OF_TEST_SET = 0.1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CUTOUT = (100, 100, 300, 300)\n",
    "CUTOUT_ARRAY = np.array([100, 300, 100, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3189,
     "status": "ok",
     "timestamp": 1672426886562,
     "user": {
      "displayName": "Jurgen Popp",
      "userId": "11749526926963684056"
     },
     "user_tz": 0
    },
    "id": "4vYurx2q4Rdj"
   },
   "outputs": [],
   "source": [
    "# initialise Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for image annotations\n",
    "# loading metadata\n",
    "file_path1 = DATA_PATH + 'combined_cat.pkl'\n",
    "file_path2 = DATA_PATH + 'zoo2LocalIdMap.pkl'\n",
    "\n",
    "df_combined_cat = (pd\n",
    "    .read_pickle(file_path1)\n",
    "    #.reset_index() \n",
    "    #.explode('false_pos_prob_stats')\n",
    ")\n",
    "\n",
    "zooToLocal = pd.read_pickle(file_path2)\n",
    "df_combined_cat['local_ids'] = zooToLocal.loc[df_combined_cat.index.get_level_values(0)].to_numpy()\n",
    "\n",
    "df_combined_cat.reset_index(inplace=True)\n",
    "\n",
    "# Filter out any bulge markings that snuck through\n",
    "df_combined_cat['is_central'] = (\n",
    "    np.abs(0.5*(df_combined_cat['x2_normed'] + df_combined_cat['x1_normed']) - 0.5) < 0.02\n",
    "    ) & (\n",
    "    np.abs(0.5*(df_combined_cat['y2_normed'] + df_combined_cat['y1_normed']) - 0.5) < 0.02\n",
    "    )\n",
    "\n",
    "# and reduce to only samples with objects\n",
    "df_combined_cat = df_combined_cat[(~df_combined_cat['is_central'] | df_combined_cat['empty'])]\n",
    "\n",
    "# stick to sizes used for Zoobot training\n",
    "cutout = CUTOUT_ARRAY\n",
    "cutout_normed = CUTOUT_ARRAY/400\n",
    "\n",
    "# Convert x/y normed\n",
    "pad = 0.05\n",
    "df_combined_cat['pad_selector'] = (\n",
    "    df_combined_cat['x1_normed'] > cutout_normed[0] + pad\n",
    "    ) & (\n",
    "    df_combined_cat['x2_normed'] < cutout_normed[1] - pad\n",
    "    ) & (\n",
    "    df_combined_cat['y1_normed'] > cutout_normed[2] + pad\n",
    "    ) & (\n",
    "    df_combined_cat['y2_normed'] < cutout_normed[3] - pad\n",
    "    )\n",
    "\n",
    "df_combined_cat = df_combined_cat[(df_combined_cat['pad_selector'] | df_combined_cat['empty'])]\n",
    "\n",
    "def convert_x_normed(x_normed):\n",
    "    x_normed = (x_normed - cutout_normed[0]) / (cutout_normed[1] - cutout_normed[0])\n",
    "    return x_normed\n",
    "\n",
    "def convert_y_normed(y_normed):\n",
    "    y_normed = (y_normed - cutout_normed[2]) / (cutout_normed[3] - cutout_normed[2])\n",
    "    return y_normed\n",
    "\n",
    "df_combined_cat['x1_normed'] = df_combined_cat.apply(lambda x: convert_x_normed(x['x1_normed']), axis=1)\n",
    "df_combined_cat['x2_normed'] = df_combined_cat.apply(lambda x: convert_x_normed(x['x2_normed']), axis=1)\n",
    "df_combined_cat['y1_normed'] = df_combined_cat.apply(lambda y: convert_y_normed(y['y1_normed']), axis=1)\n",
    "df_combined_cat['y2_normed'] = df_combined_cat.apply(lambda y: convert_y_normed(y['y2_normed']), axis=1)\n",
    "\n",
    "df_combined_cat['x1'] = df_combined_cat['x1_normed'] * (cutout[1] - cutout[0])\n",
    "df_combined_cat['x2'] = df_combined_cat['x2_normed'] * (cutout[1] - cutout[0])\n",
    "df_combined_cat['y1'] = df_combined_cat['y1_normed'] * (cutout[3] - cutout[2])\n",
    "df_combined_cat['y2'] = df_combined_cat['y2_normed'] * (cutout[3] - cutout[2])\n",
    "\n",
    "# Check, if image exists\n",
    "df_combined_cat['filename'] = IMAGE_PATH + df_combined_cat['local_ids'].apply(str) + '.png'\n",
    "df_combined_cat['file_exists'] = (df_combined_cat['filename']).apply(os.path.exists)\n",
    "\n",
    "# labels\n",
    "# 0 - background\n",
    "# 1 - Clump \n",
    "# 2 - Odd Clump\n",
    "# 3 - Improbable Clump\n",
    "# 4 - Odd Improbable Clump\n",
    "df_combined_cat['is_odd'] = np.where(df_combined_cat['mean_tool'] > 0.5, True, False)\n",
    "df_combined_cat['is_improbable'] = np.where(df_combined_cat['false_pos_prob'] > 0.7, True, False)\n",
    "\n",
    "df_combined_cat['label'] = np.select(\n",
    "    [\n",
    "        (~df_combined_cat['empty']) & (~df_combined_cat['is_odd']) & (~df_combined_cat['is_improbable']),\n",
    "        (~df_combined_cat['empty']) & (df_combined_cat['is_odd']) & (~df_combined_cat['is_improbable']),\n",
    "        (~df_combined_cat['empty']) & (~df_combined_cat['is_odd']) & (df_combined_cat['is_improbable']),\n",
    "        (~df_combined_cat['empty']) & (df_combined_cat['is_odd']) & (df_combined_cat['is_improbable']),\n",
    "    ], \n",
    "    [\n",
    "        1,\n",
    "        2,\n",
    "        2, #3\n",
    "        2, #4\n",
    "    ],\n",
    "    default = None\n",
    ")\n",
    "\n",
    "df_combined_cat['label_text'] = np.select(\n",
    "    [\n",
    "        (~df_combined_cat['empty']) & (~df_combined_cat['is_odd']) & (~df_combined_cat['is_improbable']),\n",
    "        (~df_combined_cat['empty']) & (df_combined_cat['is_odd']) & (~df_combined_cat['is_improbable']),\n",
    "        (~df_combined_cat['empty']) & (~df_combined_cat['is_odd']) & (df_combined_cat['is_improbable']),\n",
    "        (~df_combined_cat['empty']) & (df_combined_cat['is_odd']) & (df_combined_cat['is_improbable']),\n",
    "    ], \n",
    "    [\n",
    "        b'clumpy',\n",
    "        b'clumpy, odd',\n",
    "        b'clumpy, odd', # b'clumpy, improbable',\n",
    "        b'clumpy, odd', # b'clumpy, odd and improbable',\n",
    "    ],\n",
    "    default = ''\n",
    ")\n",
    "\n",
    "# get train and validation samples\n",
    "unique_ids = df_combined_cat[df_combined_cat['file_exists']]['image_id'].unique()\n",
    "# unique_ids = unique_ids[:500] # for prototyping\n",
    "\n",
    "train_ids, val_ids = train_test_split(unique_ids, test_size=SIZE_OF_VALIDATION_SET + SIZE_OF_TEST_SET, random_state=42)\n",
    "df_combined_cat = df_combined_cat[df_combined_cat['file_exists']]\n",
    "\n",
    "df_combined_cat = df_combined_cat[\n",
    "    ['image_id', 'local_ids', 'filename', 'label', 'label_text',\n",
    "    # 'x1_normed', 'x2_normed', 'y1_normed', 'y2_normed']\n",
    "    'x1', 'x2', 'y1', 'y2']\n",
    "]\n",
    "\n",
    "imageGroups_train = df_combined_cat[df_combined_cat['image_id'].isin(train_ids)]\n",
    "imageGroups_valid = df_combined_cat[df_combined_cat['image_id'].isin(val_ids)]\n",
    "\n",
    "imageGroups_train = imageGroups_train.set_index(['image_id', 'local_ids', 'filename', 'label'])\n",
    "imageGroups_valid = imageGroups_valid.set_index(['image_id', 'local_ids', 'filename', 'label'])\n",
    "\n",
    "imageGroups_train.reset_index(inplace=True)\n",
    "imageGroups_valid.reset_index(inplace=True)\n",
    "\n",
    "epochs = 80\n",
    "print('Size of train-set: {}, Size of validation-set: {}'.format(len(imageGroups_train),len(imageGroups_valid)))\n",
    "print('So, for {} epochs we need {} steps.'.format(epochs, (len(imageGroups_train)+len(imageGroups_valid)/BATCH_SIZE*epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dfs\n",
    "imageGroups_train.to_pickle('imageGroups_train.pkl')\n",
    "imageGroups_valid.to_pickle('imageGroups_valid.pkl')\n",
    "imageGroups_train.to_csv('imageGroups_train.csv')\n",
    "imageGroups_valid.to_csv('imageGroups_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    augs = []\n",
    "\n",
    "    augs.append(T.PILToTensor())\n",
    "    augs.append(T.ConvertImageDtype(torch.float))\n",
    "    \n",
    "    if train:\n",
    "        augs.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    return T.Compose(augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class and defined transformations\n",
    "import SDSSGalaxyDataset\n",
    "\n",
    "dataset_train = SDSSGalaxyDataset.SDSSGalaxyDataset(\n",
    "    dataframe=imageGroups_train,\n",
    "    image_dir=IMAGE_PATH,\n",
    "    cutout=CUTOUT,\n",
    "    colour=True,\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "dataset_validation = SDSSGalaxyDataset.SDSSGalaxyDataset(\n",
    "    dataframe=imageGroups_valid,\n",
    "    image_dir=IMAGE_PATH,\n",
    "    cutout=CUTOUT,\n",
    "    colour=True,\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_validation, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(\"Count: {} are training and {} validation\".format(len(dataset_train), len(dataset_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_data_loader))\n",
    "images = list(image.to(TORCH_DEVICE) for image in images)\n",
    "targets = [{k: v.to(TORCH_DEVICE) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,  # the image is in RGB, convert to BGR for cv2 annotations\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (0, 0, 255), 1)\n",
    "    plt.imshow(sample[:, :, ::-1])\n",
    "    # plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1672212957006,
     "user": {
      "displayName": "Jurgen Popp",
      "userId": "11749526926963684056"
     },
     "user_tz": 0
    },
    "id": "6MrzmG5hm1ch"
   },
   "outputs": [],
   "source": [
    "def get_model(num_classes=2, trainable_layers=0):\n",
    "    import copy_zoobot_weights\n",
    "\n",
    "    # load an object detection model pre-trained for Zoobot\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights='COCO_V1',\n",
    "        trainable_backbone_layers=3\n",
    "    )\n",
    "\n",
    "    model = copy_zoobot_weights.copy_Zoobot_weights_to_Resnet(\n",
    "        model=model, \n",
    "        ckpt_path=CKPT_PATH + CKPT_NAME,\n",
    "        device=TORCH_DEVICE,\n",
    "        trainable_layers=trainable_layers\n",
    "    )\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 120\n",
    "\n",
    "# get the model, all pretrained layers from the backbone CNN are freezed\n",
    "frcnn_model = get_model(num_classes=3, trainable_layers=0)\n",
    "\n",
    "# move model to the right device\n",
    "frcnn_model = frcnn_model.to(TORCH_DEVICE)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in frcnn_model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(\n",
    "        frcnn_model, \n",
    "        optimizer, \n",
    "        train_data_loader, \n",
    "        TORCH_DEVICE, \n",
    "        epoch, \n",
    "        print_freq=10,\n",
    "        scaler=None,\n",
    "        tb_writer=writer\n",
    "    )\n",
    "    \n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # evaluate on the test dataset\n",
    "    evaluate(\n",
    "        frcnn_model, \n",
    "        valid_data_loader, \n",
    "        device=TORCH_DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interference\n",
    "loaded_model = get_model(num_classes = 3)\n",
    "loaded_model.load_state_dict(torch.load(\n",
    "    './models/FasterRCNN_Zoobot_weights.pth',\n",
    "    map_location=torch.device(TORCH_DEVICE)\n",
    "))\n",
    "\n",
    "#put the model in evaluation mode\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(valid_data_loader))\n",
    "images = list(image.to(TORCH_DEVICE) for image in images)\n",
    "targets = [{k: v.to(TORCH_DEVICE) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = loaded_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(BATCH_SIZE):\n",
    "    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    pred_boxes = prediction[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    score = prediction[i]['scores'].cpu().numpy()\n",
    "    sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,  # the image is in RGB, convert to BGR for cv2 annotations\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (0, 0, 255), 1)\n",
    "\n",
    "    for idx, box in enumerate(pred_boxes):\n",
    "        if score[idx] > 0.5:\n",
    "            cv2.rectangle(sample,  # the image is in RGB, convert to BGR for cv2 annotations\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (0, 255, 255), 1)\n",
    "\n",
    "    plt.imshow(sample[:, :, ::-1])\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c186b54c8cf1ccf367fb8bdf3e071efe85839a5daed090332edb040d14e1fa50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
