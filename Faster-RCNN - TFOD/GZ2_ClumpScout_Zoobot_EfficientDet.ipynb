{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14979,"status":"ok","timestamp":1673037595903,"user":{"displayName":"Jurgen Popp","userId":"11749526926963684056"},"user_tz":0},"id":"kar9kk3kD2gP","outputId":"3babe952-d334-4193-aaec-591e927a08b1"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import pathlib\n","import os\n","import io\n","import matplotlib.pyplot as plt\n","import functools\n","import time\n","\n","from object_detection import inputs\n","\n","from object_detection.model_lib_v2 import eager_train_step\n","from object_detection.model_lib_v2 import eager_eval_loop\n","from object_detection.model_lib_v2 import load_fine_tune_checkpoint\n","from object_detection.model_lib_v2 import get_filepath\n","from object_detection.model_lib_v2 import clean_temporary_directories\n","\n","from object_detection.protos import pipeline_pb2\n","\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.utils import config_util\n","\n","from object_detection.builders import dataset_builder\n","from object_detection.builders import image_resizer_builder\n","from object_detection.builders import model_builder\n","from object_detection.builders import preprocessor_builder\n","\n","from object_detection.core import standard_fields as fields\n","\n","from object_detection.exporter_lib_v2 import DetectionInferenceModule\n","\n","from zoobot.tensorflow.estimators import custom_layers, define_model\n","\n","tf.get_logger().setLevel('ERROR')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["strategy = tf.distribute.MirroredStrategy()\n","# strategy = tf.compat.v2.distribute.get_strategy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_PATH = '../RPN_Backbone_GZ2/Data/'\n","IMAGE_PATH = DATA_PATH + 'real_pngs/'\n","TFRECORDS_PATH = './Data/tf_records/'\n","PIPELINE_CONFIG_PATH = './ssd_efficientdet_d0_512x512_coco17_tpu-8.config'\n","\n","# Full pathes for config\n","LABELS_PATH = './Data/clump_label_map_reduced.pbtxt'\n","FINE_TUNE_CHECKPOINT_PATH = './pre_trained_models/EfficientDet/ckpt-0'\n","TRAIN_DATASET_PATH = './Data/tf_records/GZ2_ClumpScout_train.records-?????-of-00006'\n","EVAL_DATASET_PATH = './Data/tf_records/GZ2_ClumpScout_val.records-?????-of-00001'\n","\n","NUM_CLASSES = 2\n","\n","PER_REPLICA_BATCH_SIZE = 4\n","try:\n","    REPLICAS = strategy.num_replicas_in_sync\n","except:\n","    REPLICAS = 1\n","\n","BATCH_SIZE = PER_REPLICA_BATCH_SIZE * REPLICAS\n","\n","IMAGE_SIZE = 256\n","SCORE_THRESHOLD = 0.8\n","\n","# Training and eval directories\n","MODEL_DIR = './models/Zoobot_EfficientDetD0/'\n","OUTPUT_MODEL_DIR = MODEL_DIR + 'saved_model'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a dictionary describing the features.\n","feature_description = {\n","    'image/height': tf.io.FixedLenFeature([], tf.int64),\n","    'image/width': tf.io.FixedLenFeature([], tf.int64),\n","    'image/local_id': tf.io.FixedLenFeature([], tf.int64),\n","    'image/source_id': tf.io.FixedLenFeature([], tf.string),\n","    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n","    'image/format': tf.io.FixedLenFeature([], tf.string),\n","    'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n","    'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n","    'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n","    'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n","    'image/object/class/text': tf.io.VarLenFeature(tf.string),\n","    'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n","}\n","\n","\n","def _parse_image_function(example_proto):\n","  # Parse the input tf.train.Example proto using the dictionary above.\n","  return tf.io.parse_single_example(example_proto, feature_description)\n","\n","\n","def plot_img_with_boxes(image, classes, boxes, scores=None, axis=None, plot=True):\n","    if scores is None:\n","        scores = np.ones(len(classes))\n","        \n","    image_with_detections = image.copy()\n","    \n","    viz_utils.visualize_boxes_and_labels_on_image_array(\n","        image_with_detections,\n","        boxes,\n","        classes,\n","        scores,\n","        category_index,\n","        use_normalized_coordinates=True,\n","        max_boxes_to_draw=100,\n","        min_score_thresh=SCORE_THRESHOLD,\n","        agnostic_mode=False,\n","        line_thickness=1,\n","    )\n","    \n","    if plot:\n","        if axis is None:\n","            plt.figure(figsize=(12,12))\n","            plt.imshow(image_with_detections)\n","            plt.show()\n","        else:\n","            axis.imshow(image_with_detections)\n","    else:\n","        return image_with_detections"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set =  sorted(str(p) for p in pathlib.Path(TFRECORDS_PATH).glob('GZ2_ClumpScout_train.records*'))\n","valid_set =  sorted(str(p) for p in pathlib.Path(TFRECORDS_PATH).glob('GZ2_ClumpScout_val.records*'))\n","test_set  =  sorted(str(p) for p in pathlib.Path(TFRECORDS_PATH).glob('GZ2_ClumpScout_test.records*'))\n","\n","train_image_dataset = tf.data.TFRecordDataset(train_set)\n","validation_image_dataset = tf.data.TFRecordDataset(valid_set)\n","test_image_dataset = tf.data.TFRecordDataset(test_set)\n","\n","# Load in the labels\n","category_index = label_map_util.create_category_index_from_labelmap(\n","    LABELS_PATH,\n","    use_display_name=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot some sample images\n","iterator = iter(test_image_dataset)\n","\n","N = 16\n","fig, ax = plt.subplots(int(np.sqrt(N)), int(np.sqrt(N)), figsize=(20,20))\n","ax = ax.flatten()\n","\n","for idx in range(N):\n","    image_features_raw = next(iterator)\n","    image_features = tf.train.Example.FromString(image_features_raw.numpy())\n","    image = tf.image.decode_png(\n","        image_features.features.feature['image/encoded'].bytes_list.value[0], channels=3\n","    ).numpy()\n","    classes = image_features.features.feature['image/object/class/label'].int64_list.value[:]\n","    # [ymin, xmin, ymax, xmax]\n","    boxes = np.stack([\n","        image_features.features.feature['image/object/bbox/ymin'].float_list.value[:],\n","        image_features.features.feature['image/object/bbox/xmin'].float_list.value[:],\n","        image_features.features.feature['image/object/bbox/ymax'].float_list.value[:],\n","        image_features.features.feature['image/object/bbox/xmax'].float_list.value[:],\n","    ], -1)\n","\n","    plot_img_with_boxes(image, classes, boxes, axis=ax[idx])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model configuration\n","configs = config_util.get_configs_from_pipeline_file(PIPELINE_CONFIG_PATH)\n","\n","configs['model'].ssd.num_classes = NUM_CLASSES\n","\n","configs['model'].ssd.image_resizer.keep_aspect_ratio_resizer.min_dimension = IMAGE_SIZE\n","configs['model'].ssd.image_resizer.keep_aspect_ratio_resizer.max_dimension = IMAGE_SIZE\n","\n","configs['train_config'].sync_replicas = True if REPLICAS > 1 else False\n","configs['train_config'].replicas_to_aggregate = REPLICAS\n","configs['train_config'].batch_size = BATCH_SIZE\n","\n","configs['train_config'].fine_tune_checkpoint = FINE_TUNE_CHECKPOINT_PATH\n","configs['train_config'].fine_tune_checkpoint_type = \"detection\"\n","\n","configs['train_input_config'].label_map_path = LABELS_PATH\n","configs['train_input_config'].tf_record_input_reader.input_path[:] = TRAIN_DATASET_PATH\n","\n","configs['eval_config'].batch_size = 1\n","configs['eval_config'].metrics_set[:] = ''\n","configs['eval_config'].metrics_set.append('coco_detection_metrics')\n","\n","configs['eval_input_config'].label_map_path = LABELS_PATH\n","configs['eval_input_config'].tf_record_input_reader.input_path[:] = EVAL_DATASET_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_config = configs['model']\n","train_config = configs['train_config']\n","train_input_config = configs['train_input_config']\n","eval_config = configs['eval_config']\n","eval_input_config = configs['eval_input_configs'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build model\n","def preprocess_fn(inputs):\n","    return inputs / 255.0\n","\n","def build_model():\n","    detection_model = model_builder._build_ssd_model(\n","        ssd_config=model_config.ssd,\n","        is_training=True,\n","        add_summaries=False\n","    )\n","\n","    detection_model._feature_extractor.preprocess = preprocess_fn\n","    \n","    return detection_model\n","\n","try:\n","    with strategy.scope():\n","        detection_model = build_model()\n","except:\n","    detection_model = build_model()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training settings\n","SHAPE = (256,256)\n","learning_rate = 1e-4\n","\n","EPOCHS = 1\n","STEPS_PER_EPOCH = 15834\n","NUM_TRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\n","train_steps = NUM_TRAIN_STEPS\n","\n","RUN_EVAL = True\n","MONITOR_METRIC = 'PascalBoxes_Precision/mAP@0.5IOU'\n","ES_PATIENCE = 5\n","\n","best_metric_value = 0.0\n","not_improved = 0\n","steps_per_sec_list = []\n","\n","unpad_groundtruth_tensors = train_config.unpad_groundtruth_tensors\n","add_regularization_loss = train_config.add_regularization_loss\n","\n","clip_gradients_value = None\n","if train_config.gradient_clipping_by_norm > 0:\n","    clip_gradients_value = train_config.gradient_clipping_by_norm\n","\n","config_util.update_fine_tune_checkpoint_type(train_config)\n","fine_tune_checkpoint_type = train_config.fine_tune_checkpoint_type\n","fine_tune_checkpoint_version = train_config.fine_tune_checkpoint_version"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    def train_dataset_fn(input_context):\n","    \n","        def transform_input_data_fn(tensor_dict):\n","            data_augmentation_options = [\n","                preprocessor_builder.build(step)\n","                for step in train_config.data_augmentation_options\n","            ]\n","            data_augmentation_fn = functools.partial(\n","                inputs.augment_input_data,\n","                data_augmentation_options=data_augmentation_options\n","            )\n","    \n","            image_resizer_config = model_config.ssd.image_resizer\n","            image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n","            \n","            transform_data_fn = functools.partial(\n","                inputs.transform_input_data, \n","                model_preprocess_fn=detection_model.preprocess,\n","                image_resizer_fn=image_resizer_fn,\n","                num_classes=NUM_CLASSES,\n","                data_augmentation_fn=data_augmentation_fn,\n","                merge_multiple_boxes=False,\n","                use_multiclass_scores=False\n","            )\n","    \n","            tensor_dict = inputs.pad_input_data_to_static_shapes(\n","                tensor_dict=transform_data_fn(tensor_dict),\n","                max_num_boxes=train_input_config.max_number_of_boxes,\n","                num_classes=NUM_CLASSES,\n","                spatial_image_shape=SHAPE\n","            )\n","    \n","            return (inputs._get_features_dict(tensor_dict, False),\n","                    inputs._get_labels_dict(tensor_dict))\n","    \n","        train_input = dataset_builder.build(\n","            input_reader_config=train_input_config,\n","            transform_input_data_fn=transform_input_data_fn,\n","            batch_size=train_config.batch_size,\n","            input_context=input_context,\n","        )\n","        train_input = train_input.repeat()    \n","    \n","        return train_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    def eval_dataset_fn(input_context):\n","    \n","        def transform_input_data_fn(tensor_dict):\n","            image_resizer_config = model_config.ssd.image_resizer\n","            image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n","    \n","            transform_data_fn = functools.partial(\n","                inputs.transform_input_data, \n","                model_preprocess_fn=detection_model.preprocess,\n","                image_resizer_fn=image_resizer_fn,\n","                num_classes=NUM_CLASSES,\n","                merge_multiple_boxes=False,\n","                use_multiclass_scores=False,\n","                retain_original_image=eval_config.retain_original_images,\n","                retain_original_image_additional_channels=eval_config.retain_original_image_additional_channels\n","            )\n","    \n","            tensor_dict = inputs.pad_input_data_to_static_shapes(\n","                tensor_dict=transform_data_fn(tensor_dict),\n","                max_num_boxes=eval_input_config.max_number_of_boxes,\n","                num_classes=NUM_CLASSES,\n","                spatial_image_shape=SHAPE\n","            )\n","    \n","            return (inputs._get_features_dict(tensor_dict, False),\n","                    inputs._get_labels_dict(tensor_dict))\n","    \n","        eval_input = dataset_builder.build(\n","            eval_input_config,\n","            transform_input_data_fn=transform_input_data_fn,\n","            batch_size=eval_config.batch_size,\n","            input_context=input_context,\n","        )\n","    \n","        return eval_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    train_input = strategy.experimental_distribute_datasets_from_function(train_dataset_fn)\n","    eval_input = strategy.experimental_distribute_datasets_from_function(eval_dataset_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    train_input_iter = iter(train_input)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load pre-trained COCO checkpoint\n","with strategy.scope():\n","    global_step = tf.Variable(\n","        0,\n","        trainable=False,\n","        dtype=tf.compat.v2.dtypes.int64,name='global_step',\n","        aggregation=tf.compat.v2.VariableAggregation.ONLY_FIRST_REPLICA\n","    )\n","    \n","    checkpointed_step = int(global_step.value())\n","    logged_step = int(global_step.value())\n","    total_loss = 0\n","\n","    if train_config.fine_tune_checkpoint:\n","        load_fine_tune_checkpoint(\n","            model=detection_model,\n","            checkpoint_path=train_config.fine_tune_checkpoint,\n","            checkpoint_type=fine_tune_checkpoint_type,\n","            checkpoint_version=fine_tune_checkpoint_version,\n","            run_model_on_dummy_input=False,\n","            input_dataset=train_input,\n","            unpad_groundtruth_tensors=unpad_groundtruth_tensors\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load Zoobot\n","INITIAL_SIZE = 300\n","CROP_SIZE = int(INITIAL_SIZE * 0.75)\n","RESIZE_SIZE = 224   # Zoobot, as pretrained, expects 224x224 images\n","\n","checkpoint_dir = './pre_trained_models/Zoobot_EfficientnetB0_colour/'\n","checkpoint_loc = os.path.join(checkpoint_dir, 'checkpoint')\n","\n","conv_base = define_model.load_model(\n","    checkpoint_loc,  # loading pretrained model as above\n","    expect_partial=True,  # ignores some optimizer warnings\n","    include_top=False,  # do not include the head used for GZ DECaLS, this time - we will add our own head\n","    input_size=INITIAL_SIZE,  # the preprocessing above did not change size\n","    crop_size=CROP_SIZE,  # model augmentation layers apply a crop...\n","    resize_size=RESIZE_SIZE,  # ...and then apply a resize\n","    output_dim=None,\n","    channels=3\n",")\n","\n","inputs = tf.keras.Input(shape=(INITIAL_SIZE, INITIAL_SIZE, 3))\n","x = conv_base(inputs)\n","outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","zoobot = tf.keras.Model(inputs, outputs)\n","\n","zoobot.compile(\n","    loss='binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["efficientnet_zoobot = zoobot.get_layer('sequential').get_layer('sequential_1').get_layer('efficientnet-b0')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# assign the Zoobot weights to the EfficientDet detection model\n","for i, weight in enumerate(efficientnet_zoobot.weights):\n","    detection_model.feature_extractor.classification_backbone.weights[i].assign(weight)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    if callable(learning_rate):\n","        learning_rate_fn = learning_rate\n","    else:\n","        learning_rate_fn = lambda: learning_rate\n","\n","    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n","    ckpt = tf.compat.v2.train.Checkpoint(step=global_step, model=detection_model, optimizer=optimizer)\n","    manager_dir = get_filepath(strategy, MODEL_DIR)\n","    manager = tf.compat.v2.train.CheckpointManager(ckpt, manager_dir, max_to_keep=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reduce_dict(strategy, reduction_dict, reduction_op):\n","  # scaling of the loss and switch this to a ReduceOp.Mean\n","  return {\n","      name: strategy.reduce(reduction_op, loss, axis=None)\n","      for name, loss in reduction_dict.items()\n","  }\n","\n","\n","# Training\n","with strategy.scope():\n","    def train_step_fn(features, labels):\n","        losses_dict = eager_train_step(\n","            detection_model,\n","            features,\n","            labels,\n","            unpad_groundtruth_tensors,\n","            optimizer,\n","            training_step=global_step,\n","            #learning_rate=learning_rate_fn(),\n","            add_regularization_loss=add_regularization_loss,\n","            clip_gradients_value=clip_gradients_value,\n","            num_replicas=REPLICAS\n","        )\n","        global_step.assign_add(1)\n","        \n","        return losses_dict\n","\n","\n","    def _sample_and_train(strategy, train_step_fn, data_iterator):\n","        features, labels = data_iterator.next()\n","        per_replica_losses_dict = strategy.run(train_step_fn, args=(features, labels))\n","        \n","        return reduce_dict(strategy, per_replica_losses_dict, tf.distribute.ReduceOp.SUM)\n","\n","\n","    @tf.function\n","    def _dist_train_step(data_iterator):\n","        return _sample_and_train(strategy, train_step_fn, data_iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training Loop\n","last_step_time = time.time()\n","\n","for _ in range(global_step.value(), train_steps):\n","    with strategy.scope():\n","        loss = _dist_train_step(train_input_iter)\n","        time_taken = time.time() - last_step_time\n","        last_step_time = time.time()\n","        steps_per_sec = 1.0 / time_taken\n","        steps_per_sec_list.append(steps_per_sec)\n","        total_loss += loss\n","        \n","    if int(global_step.value()) % STEPS_PER_EPOCH == 0:\n","        if not RUN_EVAL:\n","            print('Epoch {} [ETA {:.2f}s] loss={:.3f}'.format(\n","                  int(global_step.value()) // STEPS_PER_EPOCH,\n","                  time_taken * STEPS_PER_EPOCH,\n","                  total_loss / STEPS_PER_EPOCH))\n","        else:\n","            eval_global_step = tf.compat.v2.Variable(0, trainable=False, dtype=tf.compat.v2.dtypes.int64)\n","            eval_metrics = eager_eval_loop(\n","                detection_model,\n","                configs,\n","                eval_input,\n","                global_step=eval_global_step\n","            )\n","\n","            print('Epoch {} [ETA {:.2f}s] loss={:.3f} mAP@.5={:.3f}'.format(\n","                  int(global_step.value()) // STEPS_PER_EPOCH,\n","                  time_taken * STEPS_PER_EPOCH,\n","                  total_loss / STEPS_PER_EPOCH,\n","                  eval_metrics[MONITOR_METRIC]))\n","\n","            if eval_metrics[MONITOR_METRIC] > best_metric_value:\n","                best_metric_value = eval_metrics[MONITOR_METRIC]\n","                manager.save()\n","                not_improved = 0\n","            else:\n","                not_improved += 1\n","\n","            if not_improved >= ES_PATIENCE:\n","                print(f\"Early stopping at epoch {int(global_step.value()) // STEPS_PER_EPOCH}\")\n","                break\n","            \n","        total_loss = 0\n","\n","clean_temporary_directories(strategy, manager_dir)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"env_tf","language":"python","name":"env_tf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"}}},"nbformat":4,"nbformat_minor":0}
